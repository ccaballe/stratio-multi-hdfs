<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?><configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hdfs-test-1</value>
    </property>
    <property>
        <name>hadoop.common.configuration.version</name>
        <value>0.23.0</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hue.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hue.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hdfs.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.hdfs.groups</name>
        <value>*</value>
    </property>

    <property>
        <name>hadoop.proxyuser.root.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.root.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.httpfs.groups</name>
        <value>*</value>
    </property>
    <property>
        <name>hadoop.proxyuser.httpfs.hosts</name>
        <value>*</value>
    </property>
    <property>
        <name>ha.zookeeper.parent-znode</name>
        <value>/dcos-service-hdfs-test-1</value>
    </property>
    <property>
        <name>ha.zookeeper.quorum</name>
        <value>master.mesos:2181</value>
    </property>
    <property>
        <name>ipc.client.connect.max.retries</name>
        <value>300</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/var/lib/hadoop/cache/</value>
    </property>
    <property>
        <name>hadoop.http.filter.initializers</name>
        <value>org.apache.hadoop.security.AuthenticationFilterInitializer</value>
    </property>
    <property>
        <name>hadoop.security.authorization</name>
        <value>false</value>
    </property>
    <property>
        <name>hadoop.security.instrumentation.requires.admin</name>
        <value>false</value>
    </property>
    <property>
        <name>hadoop.security.authentication</name>
        <value>kerberos</value>
    </property>
    <property>
        <name>hadoop.security.groups.cache.secs</name>
        <value>300</value>
    </property>
    <property>
        <name>hadoop.security.groups.negative-cache.secs</name>
        <value>30</value>
    </property>
    <property>
        <name>hadoop.security.groups.cache.warn.after.ms</name>
        <value>5000</value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.url</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.ssl</name>
        <value>false</value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.ssl.keystore</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.ssl.keystore.password.file</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.bind.user</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.bind.password.file</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.base</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.search.filter.user</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.search.filter.group</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.search.attr.member</name>
        <value>member</value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.search.attr.group.name</name>
        <value>cn</value>
    </property>
    <property>
        <name>hadoop.security.group.mapping.ldap.directory.search.timeout</name>
        <value>10000</value>
    </property>
    <property>
        <name>hadoop.security.service.user.name.key</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.security.uid.cache.secs</name>
        <value>14400</value>
    </property>
    <property>
        <name>hadoop.rpc.protection</name>
        <value>privacy</value>
    </property>
    <property>
        <name>hadoop.security.saslproperties.resolver.class</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.work.around.non.threadsafe.getpwuid</name>
        <value>false</value>
    </property>
    <property>
        <name>hadoop.kerberos.kinit.command</name>
        <value>kinit</value>
    </property>

    <property>
        <name>hadoop.security.auth_to_local</name>
        <value>
            RULE:[1:$1@$0](.*@STRATIO.COM)s/@STRATIO.COM//
            RULE:[2:$1@$0](.*@STRATIO.COM)s/@STRATIO.COM//
            DEFAULT
        </value>
    </property>


    <property>
        <name>io.file.buffer.size</name>
        <value>4096</value>
    </property>
    <property>
        <name>io.bytes.per.checksum</name>
        <value>512</value>
    </property>
    <property>
        <name></name>
        <value>512</value>
    </property>
    <property>
        <name>io.skip.checksum.errors</name>
        <value>false</value>
    </property>
    <property>
        <name>io.compression.codecs</name>
        <value></value>
    </property>
    <property>
        <name>io.compression.codec.bzip2.library</name>
        <value>system-native</value>
    </property>
    <property>
        <name>io.serializations</name>
        <value>org.apache.hadoop.io.serializer.WritableSerialization,org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization</value>
    </property>
    <property>
        <name>io.seqfile.local.dir</name>
        <value>${hadoop.tmp.dir}/io/local</value>
    </property>
    <property>
        <name>io.map.index.skip</name>
        <value>0</value>
    </property>
    <property>
        <name>io.map.index.interval</name>
        <value>128</value>
    </property>
    <property>
        <name>fs.trash.interval</name>
        <value>0</value>
    </property>
    <property>
        <name>fs.trash.checkpoint.interval</name>
        <value>0</value>
    </property>
    <property>
        <name>fs.AbstractFileSystem.file.impl</name>
        <value>org.apache.hadoop.fs.local.LocalFs</value>
    </property>
    <property>
        <name>fs.AbstractFileSystem.har.impl</name>
        <value>org.apache.hadoop.fs.HarFs</value>
    </property>
    <property>
        <name>fs.AbstractFileSystem.hdfs.impl</name>
        <value>org.apache.hadoop.fs.Hdfs</value>
    </property>
    <property>
        <name>fs.AbstractFileSystem.viewfs.impl</name>
        <value>org.apache.hadoop.fs.viewfs.ViewFs</value>
    </property>
    <property>
        <name>fs.ftp.host</name>
        <value>0.0.0.0</value>
    </property>
    <property>
        <name>fs.ftp.host.port</name>
        <value>21</value>
    </property>
    <property>
        <name>fs.df.interval</name>
        <value>60000</value>
    </property>
    <property>
        <name>fs.du.interval</name>
        <value>600000</value>
    </property>
    <property>
        <name>fs.s3.block.size</name>
        <value>67108864</value>
    </property>
    <property>
        <name>fs.s3.buffer.dir</name>
        <value>${hadoop.tmp.dir}/s3</value>
    </property>
    <property>
        <name>fs.s3.maxRetries</name>
        <value>4</value>
    </property>
    <property>
        <name>fs.s3.sleepTimeSeconds</name>
        <value>10</value>
    </property>
    <property>
        <name>fs.swift.impl</name>
        <value>org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem</value>
    </property>
    <property>
        <name>fs.automatic.close</name>
        <value>true</value>
    </property>
    <property>
        <name>fs.s3n.block.size</name>
        <value>67108864</value>
    </property>
    <property>
        <name>fs.s3n.multipart.uploads.enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>fs.s3n.multipart.uploads.block.size</name>
        <value>67108864</value>
    </property>
    <property>
        <name>fs.s3n.multipart.copy.block.size</name>
        <value>5368709120</value>
    </property>
    <property>
        <name>fs.s3n.server-side-encryption-algorithm</name>
        <value></value>
    </property>
    <property>
        <name>fs.s3a.access.key</name>
        <value></value>
    </property>
    <property>
        <name>fs.s3a.secret.key</name>
        <value></value>
    </property>
    <property>
        <name>fs.s3a.connection.maximum</name>
        <value>15</value>
    </property>
    <property>
        <name>fs.s3a.connection.ssl.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>fs.s3a.attempts.maximum</name>
        <value>10</value>
    </property>
    <property>
        <name>fs.s3a.connection.timeout</name>
        <value>5000</value>
    </property>
    <property>
        <name>fs.s3a.paging.maximum</name>
        <value>5000</value>
    </property>
    <property>
        <name>fs.s3a.multipart.size</name>
        <value>104857600</value>
    </property>
    <property>
        <name>fs.s3a.multipart.threshold</name>
        <value>2147483647</value>
    </property>
    <property>
        <name>fs.s3a.acl.default</name>
        <value></value>
    </property>
    <property>
        <name>fs.s3a.multipart.purge</name>
        <value>false</value>
    </property>
    <property>
        <name>fs.s3a.multipart.purge.age</name>
        <value>86400</value>
    </property>
    <property>
        <name>fs.s3a.buffer.dir</name>
        <value>${hadoop.tmp.dir}/s3a</value>
    </property>
    <property>
        <name>fs.s3a.impl</name>
        <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
    </property>
    <property>
        <name>io.seqfile.compress.blocksize</name>
        <value>1000000</value>
    </property>
    <property>
        <name>io.seqfile.lazydecompress</name>
        <value>true</value>
    </property>
    <property>
        <name>io.seqfile.sorter.recordlimit</name>
        <value>1000000</value>
    </property>
    <property>
        <name>io.mapfile.bloom.size</name>
        <value>1048576</value>
    </property>
    <property>
        <name>io.mapfile.bloom.error.rate</name>
        <value>0.005</value>
    </property>
    <property>
        <name>hadoop.util.hash.type</name>
        <value>murmur</value>
    </property>
    <property>
        <name>ipc.client.idlethreshold</name>
        <value>4000</value>
    </property>
    <property>
        <name>ipc.client.kill.max</name>
        <value>10</value>
    </property>
    <property>
        <name>ipc.client.connection.maxidletime</name>
        <value>10000</value>
    </property>
    <property>
        <name>ipc.client.connect.max.retries</name>
        <value>300</value>
    </property>
    <property>
        <name>ipc.client.connect.retry.interval</name>
        <value>1000</value>
    </property>
    <property>
        <name>ipc.client.connect.timeout</name>
        <value>90000</value>
    </property>
    <property>
        <name>ipc.client.connect.max.retries.on.timeouts</name>
        <value>45</value>
    </property>
    <property>
        <name>ipc.server.listen.queue.size</name>
        <value>45</value>
    </property>
    <property>
        <name>hadoop.security.impersonation.provider.class</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.rpc.socket.factory.class.default</name>
        <value>org.apache.hadoop.net.StandardSocketFactory</value>
    </property>
    <property>
        <name>hadoop.rpc.socket.factory.class.ClientProtocol</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.socks.server</name>
        <value></value>
    </property>


    <property>
        <name>file.stream-buffer-size</name>
        <value>4096</value>
    </property>
    <property>
        <name>file.bytes-per-checksum</name>
        <value>512</value>
    </property>
    <property>
        <name>file.client-write-packet-size</name>
        <value>65536</value>
    </property>
    <property>
        <name>file.blocksize</name>
        <value>67108864</value>
    </property>
    <property>
        <name>file.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>s3.stream-buffer-size</name>
        <value>4096</value>
    </property>
    <property>
        <name>s3.bytes-per-checksum</name>
        <value>512</value>
    </property>
    <property>
        <name>s3.client-write-packet-size</name>
        <value>65536</value>
    </property>
    <property>
        <name>s3.blocksize</name>
        <value>67108864</value>
    </property>
    <property>
        <name>s3.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>s3native.stream-buffer-size</name>
        <value>4096</value>
    </property>
    <property>
        <name>s3native.bytes-per-checksum</name>
        <value>512</value>
    </property>
    <property>
        <name>s3native.client-write-packet-size</name>
        <value>65536</value>
    </property>
    <property>
        <name>s3native.blocksize</name>
        <value>67108864</value>
    </property>
    <property>
        <name>s3native.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>ftp.stream-buffer-size</name>
        <value>4096</value>
    </property>
    <property>
        <name>ftp.bytes-per-checksum</name>
        <value>512</value>
    </property>
    <property>
        <name>ftp.client-write-packet-size</name>
        <value>65536</value>
    </property>
    <property>
        <name>ftp.blocksize</name>
        <value>67108864</value>
    </property>
    <property>
        <name>ftp.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>tfile.io.chunk.size</name>
        <value>1048576</value>
    </property>
    <property>
        <name>tfile.fs.output.buffer.size</name>
        <value>262144</value>
    </property>
    <property>
        <name>tfile.fs.input.buffer.size</name>
        <value>262144</value>
    </property>
    <property>
        <name>hadoop.http.authentication.type</name>
        <value>kerberos</value>
    </property>
    <property>
        <name>hadoop.http.authentication.token.validity</name>
        <value>36000</value>
    </property>
    <property>
        <name>hadoop.http.authentication.signature.secret.file</name>
        <value>${user.home}/hadoop-http-auth-signature-secret</value>
    </property>
    <property>
        <name>hadoop.http.authentication.cookie.domain</name>
        <value>labs.stratio.com</value>
    </property>
    <property>
        <name>hadoop.http.authentication.simple.anonymous.allowed</name>
        <value>false</value>
    </property>


    <property>
        <name>dfs.permissions.enabled</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>true</value>
    </property>
    <property>
        <name>dfs.namenode.inode.attributes.provider.class</name>
        <value>com.stratio.gosec.dyplon.plugins.hdfs.Auth</value>
    </property>

    <property>
        <name>hadoop.http.authentication.kerberos.principal</name>
        <value>HTTP/*@STRATIO.COM</value>
    </property>

    <property>
        <name>hadoop.http.authentication.kerberos.keytab</name>
        <value>/sandboxpath/certificates/HTTP.name-0-zkfc.hdfs-test-1.mesos.keytab</value>
    </property>

    <property>
        <name>dfs.ha.fencing.ssh.connect-timeout</name>
        <value>30000</value>
    </property>
    <property>
        <name>dfs.ha.fencing.ssh.private-key-files</name>
        <value></value>
    </property>
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>dr.who</value>
    </property>
    <property>
        <name>ha.zookeeper.session-timeout.ms</name>
        <value>5000</value>
    </property>
    <property>
        <name>hadoop.ssl.keystores.factory.class</name>
        <value>org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory</value>
    </property>
    <property>
        <name>hadoop.ssl.require.client.cert</name>
        <value>false</value>
    </property>
    <property>
        <name>hadoop.jetty.logs.serve.aliases</name>
        <value>false</value>
    </property>
    <property>
        <name>fs.permissions.umask-mode</name>
        <value>000</value>
    </property>
    <property>
        <name>ha.health-monitor.connect-retry-interval.ms</name>
        <value>1000</value>
    </property>
    <property>
        <name>ha.health-monitor.check-interval.ms</name>
        <value>1000</value>
    </property>
    <property>
        <name>ha.health-monitor.sleep-after-disconnect.ms</name>
        <value>1000</value>
    </property>
    <property>
        <name>ha.health-monitor.rpc-timeout.ms</name>
        <value>45000</value>
    </property>
    <property>
        <name>ha.failover-controller.new-active.rpc-timeout.ms</name>
        <value>60000</value>
    </property>
    <property>
        <name>ha.failover-controller.graceful-fence.rpc-timeout.ms</name>
        <value>5000</value>
    </property>
    <property>
        <name>ha.failover-controller.graceful-fence.connection.retries</name>
        <value>1</value>
    </property>
    <property>
        <name>ha.failover-controller.cli-check.rpc-timeout.ms</name>
        <value>20000</value>
    </property>
    <property>
        <name>ipc.client.fallback-to-simple-auth-allowed</name>
        <value>false</value>
    </property>
    <property>
        <name>fs.client.resolve.remote.symlinks</name>
        <value>true</value>
    </property>
    <property>
        <name>nfs.exports.allowed.hosts</name>
        <value>* rw</value>
    </property>
    <property>
        <name>hadoop.user.group.static.mapping.overrides</name>
        <value></value>
    </property>
    <property>
        <name>rpc.metrics.quantile.enable</name>
        <value>false</value>
    </property>
    <property>
        <name>rpc.metrics.percentiles.intervals</name>
        <value></value>
    </property>

    <property>
        <name>hadoop.ssl.enabled.protocols</name>
        <value>TLSv1.2</value>
    </property>
    <property>
        <name>hadoop.ssl.hostname.verifier</name>
        <value>ALLOW_ALL</value>
    </property>
    <property> <!-- The ZKFC nodes use this property to verify they are connecting to the namenode with the expected principal. -->
        <name>hadoop.security.service.user.name.key.pattern</name>
        <value>hdfs/*@STRATIO.COM</value>
    </property>
    <property>
        <name>hadoop.security.authentication</name>
        <value>kerberos</value>
    </property>
    <property>
        <name>hadoop.security.authorization</name>
        <value>true</value>
    </property>
</configuration>